<!doctype html>
<html lang="en">
<meta charset="utf-8">
<title>2. Autograd</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+3:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
<style>
    body {
        font-size: 1.4em;
        font-family: 'Source Sans 3', 'Hoefler Text', Georgia, 'Times New Roman', Times, serif;
        max-width: 900px;
        margin: 1em auto 10em;
    }

    :focus {
        outline-width: 2px;
        outline-style: solid;
        outline-color: #00f;
        border-radius: 2px;
        border-color: transparent;
    }

    [aria-label] {
        position: relative;
    }

    [aria-label]:focus-within::before {
        content: attr(aria-label);
        position: absolute;
        bottom: -20px;
        font-size: 12px;
        /* right: 0; */
    }

    input, button {
        font-family: inherit;
        font-size: inherit;
        font-size: 0.8em;
    }

    pre, code, samp, textarea {
        font-family: 'Source Code Pro', ui-monospace, Menlo, Monaco, "Cascadia Mono", "Segoe UI Mono", "Roboto Mono", "Oxygen Mono", "Ubuntu Monospace", "Source Code Pro", "Fira Mono",  "Droid Sans Mono", "Courier New", monospace !important;
        font-size: 0.8em !important;
        background: lightgoldenrodyellow;
    }

    pre, textarea {
        overflow: auto;
        padding: 1em;
    }

    pre[data-error] {
        background: lightpink;
    }

    details {
        margin: 1em 0;
    }

    aside {
        background-color: lavender;
        padding: .5em .7em;
    }

    textarea {
        width: 100%;
        border: none;
        resize: none;
        text-wrap: nowrap;
    }

    /* nav {
        position: fixed;
        top: 0;
        left: 0;
        bottom: 0;
        background: lightgoldenrodyellow;
        padding: 1em;
        overflow: auto;
    } */
</style>
<article>
<h1>2. Autograd</h1>

<aside>
    This covers the <a href="https://www.youtube.com/watch?v=VMj-3S1tku0">building micrograd</a> video.
</aside>

<p>Manually figuring out gradients can be tedious. It’s relatively easy for a
single layer neural network, but gets more complex as we add layers. Machine
Learning libraries all have an autograd engine: you can build out a
<em>mathematical expression</em> and it will automatically be able to figure out
the gradients with respect to the variables. We’ll now build something similar,
although just for the operations we need. In the previous notebook, we just
needed gradients for <code class="language-plaintext highlighter-rouge">matMul</code> and <code class="language-plaintext highlighter-rouge">softmaxCrossEntropy</code>, so let’s start with
these operations.</p>

<p>But first, let’s build a simpler autograd engine with just two operations:
addition and multiplication of scalar numbers.</p>

<blockquote>
  <p>[Auto]grad is everything you need to build neural networks, everything else is
just for efficiency. - Andrej Karpathy</p>
</blockquote>

<h2 id="building-value">Building <code class="language-plaintext highlighter-rouge">Value</code></h2>

<p>To build the tree, we need a wrapper around values. Then we’ll need the
gradients with respect to the variables so we know how much each variable
affects the result.</p>

<textarea disabled rows='7'>
class Value {
    constructor(data) {
        this.data = data;
    }
}
const a = new Value(2);
</textarea>

<p>Nice! Now we can do some operations on these values and an example expression.</p>

<textarea disabled rows='24'>
class Value {
    constructor(data, _children = []) {
        this.data = data;
        this._prev = new Set( _children );
    }
    add(other) {
        const result = new Value(this.data + other.data, [ this, other ]);
        result._op = '+';
        return result;
    }
    mul(other) {
        const result = new Value(this.data * other.data, [ this, other ]);
        result._op = '*';
        return result;
    }
}
const a = new Value(2);
const b = new Value(-3);
const c = new Value(10);
const d = a.mul(b);
const e = d.add(c);
const f = new Value(-2);
const L = e.mul(f);
</textarea>

<p>Let’s visualize the graph to better understand it. Ignore the graph code, it’s
not so important, but all here for transparency.</p>

<textarea disabled rows='4'>
export { instance } from 'https://esm.sh/@viz-js/viz';
export { Graph } from 'https://esm.sh/@dagrejs/graphlib';
export { default as graphlibDot } from 'https://esm.sh/graphlib-dot';
</textarea>

<textarea disabled rows='75'>
const ids = new WeakMap();

function createId(node) {
    let id = ids.get(node);
    if (!id) {
        id = createId.counter++;
        ids.set(node, id);
    }
    return id;
}

createId.counter = 0;

function trace( root ) {
	const nodes = new Set();
	const edges = new Set();

	function build( node ) {
		if ( ! nodes.has( node ) ) {
			nodes.add( node );

			for ( const child of node._prev ) {
				edges.add( [ child, node ] );
				build( child );
			}
		}
	}

	build( root );

	return { nodes, edges };
}

async function drawDot(root) {
	const { nodes, edges } = trace( root );
	const graph = new Graph( { compound: true } );
    graph.setGraph({ rankdir: "LR" });

	for ( const node of nodes ) {
		node._id = createId(node);
		graph.setNode( node._id, {
            shape: 'record',
            // `grad` will be important later.
            label: [
                node.label,
                [ 'data', node.data ].join(': '),
                node.grad !== undefined ? [ 'grad', node.grad ].join(': ') : null
            ].filter( Boolean ).join(' | ')
        } );

		if ( node._op ) {
			graph.setNode( node._id + node._op, { label: node._op } );
			graph.setEdge( node._id + node._op, node._id );
		}
	}

	for ( const [ node, child ] of edges ) {
		graph.setEdge( node._id, child._id + child._op );
	}

	const viz = await instance();
    const dotString = graphlibDot.write(graph);
    return viz.renderSVGElement(dotString);
}

a.label = 'a';
b.label = 'b';
c.label = 'c';
d.label = 'd';
e.label = 'e';
f.label = 'f';
L.label = 'L';

print(await drawDot(L));
</textarea>

<p>Great, we have now implemented the forward pass for multiplication and addition.</p>

<h2 id="manual-gradient-calculation">Manual gradient calculation</h2>

<p>derivative = sensitivity.</p>

<p>Let’s add a “backward pass”: calculating the gradients for each node from the
perspective of the output <code class="language-plaintext highlighter-rouge">L</code>. The gradient of L with respect to L is 1: if you
change L by a tiny amount, well, then L changes by that tiny amount.</p>

<p>So we should always start a backward pass setting the output’s gradient to 1.</p>

<textarea disabled rows='3'>
L.grad = 1;
print(await drawDot(L));
</textarea>

<p>For multiplication <code class="language-plaintext highlighter-rouge">a*b=output</code>, the gradient of an input <code class="language-plaintext highlighter-rouge">a</code> with respect to
the <code class="language-plaintext highlighter-rouge">output</code> is the other input, <code class="language-plaintext highlighter-rouge">b</code>. <code class="language-plaintext highlighter-rouge">d(output)/da = b</code> and <code class="language-plaintext highlighter-rouge">d(output)/db = a</code>.
In other words, if you change <code class="language-plaintext highlighter-rouge">a</code> by a tiny amount, the result changes by <code class="language-plaintext highlighter-rouge">b</code>
times a tiny amount. If you change <code class="language-plaintext highlighter-rouge">b</code> by a tiny amount, the result changes by
<code class="language-plaintext highlighter-rouge">a</code> times a tiny amount.</p>

<p>If you remember your calculus, you might remember that the definition of the
derivative is the limit of <code class="language-plaintext highlighter-rouge">(f(x+h)-f(x))/h</code> as <code class="language-plaintext highlighter-rouge">h</code> goes to 0. So if we fill this in for multiplication, we get:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>((a+h)*b - a*b)/h =
(ab + bh - ab)/h =
bh/h =
b
</code></pre></div></div>

<textarea disabled rows='4'>
e.grad = f.data;
f.grad = e.data;
print(await drawDot(L));
</textarea>

<p>Now we get the most important part. We need the gradient of <code class="language-plaintext highlighter-rouge">d</code> and <code class="language-plaintext highlighter-rouge">c</code> with
respect to <code class="language-plaintext highlighter-rouge">L</code>. We actually already have some information: the derivative of
<code class="language-plaintext highlighter-rouge">e</code> with respect to <code class="language-plaintext highlighter-rouge">L</code>. If we know the gradient of <code class="language-plaintext highlighter-rouge">d</code> with respect to <code class="language-plaintext highlighter-rouge">e</code>,
we can use the chain rule:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>d(L)/d(e) * d(e)/d(c) = d(L)/d(c)
</code></pre></div></div>

<p>So what is the gradient of <code class="language-plaintext highlighter-rouge">d</code> with respect to <code class="language-plaintext highlighter-rouge">e</code>?</p>

<p>For addition, the gradient of an input <code class="language-plaintext highlighter-rouge">a</code> with respect to the <code class="language-plaintext highlighter-rouge">output</code> is 1.
<code class="language-plaintext highlighter-rouge">d(output)/da = 1</code> and <code class="language-plaintext highlighter-rouge">d(output)/db = 1</code>. In other words, if you change <code class="language-plaintext highlighter-rouge">a</code> by
a tiny amount, the result changes by 1 times a tiny amount. If you change <code class="language-plaintext highlighter-rouge">b</code> by
a tiny amount, the result changes by 1 times a tiny amount.</p>

<p>Again, following the definition of the derivative, we get:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>((a+h) + b - (a + b))/h =
(a + h + b - a - b)/h =
h/h =
1
</code></pre></div></div>

<p>So <code class="language-plaintext highlighter-rouge">d(e)/d(c)</code> is 1.</p>

<p>And using the chain rule, we get now know that <code class="language-plaintext highlighter-rouge">d(L)/d(c)</code> is <code class="language-plaintext highlighter-rouge">d(L)/d(e) *
d(e)/d(c) = d(L)/d(e) * 1</code>. Plus nodes just pass the gradient along.</p>

<p>This is the core of backpropagation: as we propagate through the nodes, we can
simply multiply the gradients together to get the gradient of the input with
respect to the output.</p>

<textarea disabled rows='4'>
c.grad = e.grad * 1;
d.grad = e.grad * 1;
print(await drawDot(L));
</textarea>

<p>And finally, we can do the same thing for <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>. The gradient of <code class="language-plaintext highlighter-rouge">a</code> with
respect to <code class="language-plaintext highlighter-rouge">L</code> is <code class="language-plaintext highlighter-rouge">d(L)/d(a) = d(L)/d(d) * d(d)/d(a) = d(d) * b</code>. And the gradient
of <code class="language-plaintext highlighter-rouge">b</code> with respect to <code class="language-plaintext highlighter-rouge">L</code> is <code class="language-plaintext highlighter-rouge">d(L)/d(b) = d(L)/d(d) * d(d)/d(b) = d(d) * a</code>.</p>

<textarea disabled rows='4'>
b.grad = d.grad * a.data;
a.grad = d.grad * b.data;
print(await drawDot(L));
</textarea>

<p>We can also numerically check our gradients. If we nudge <code class="language-plaintext highlighter-rouge">a</code> by a small amount, let’s say <code class="language-plaintext highlighter-rouge">0.001</code>, we should see the output change by the gradient times <code class="language-plaintext highlighter-rouge">0.001</code>.</p>

<textarea disabled rows='17'>
function f(h) {
    const a = new Value(2);
    a.data += h;
    const b = new Value(-3);
    const c = new Value(10);
    const d = a.mul(b);
    const e = d.add(c);
    const f = new Value(-2);
    return e.mul(f);
}

const h = 0.001;
const L1 = f(0);
const L2 = f(h);
const numericalGradient = (L2.data - L1.data) / h;
const analyticalGradient = a.grad;
</textarea>

<h2 id="lets-build-a-neuron">Let’s build a neuron</h2>

<p>These gradients are important for training neural networks, because we will
define a loss function and then calculate the gradient of the loss with respect
to each parameter in the network, which we can then use to update the parameters
to improve the accuracy.</p>

<p>Now, we don’t need to implement every single atomic operation. As long as we
know how to calculate the local derivative for a function, we can cluster it
together. <code class="language-plaintext highlighter-rouge">tanh(x)</code> is important for neural networks, because it squashes the
output of a neuron to a range between -1 and 1.</p>

<textarea disabled rows='44'>
Value.prototype.tanh = function() {
    const x = this.data;
    const t = (Math.exp(2*x)-1)/(Math.exp(2*x)+1);
    const result = new Value(t, [this]);
    result._op = 'tanh';
    return result;
}

// Inputs x1, x2.
const x1 = new Value(2);
const x2 = new Value(0);
// Weights w1, w2.
const w1 = new Value(-3);
const w2 = new Value(1);
// Bias b.
const b = new Value(6.8813735870195432);
// Output.
const x1w1 = x1.mul(w1);
const x2w2 = x2.mul(w2);
const n = x1w1.add(x2w2).add(b);
const o = n.tanh();

x1.label = 'x1';
x2.label = 'x2';
w1.label = 'w1';
w2.label = 'w2';
b.label = 'bias';

// We should always set the output's gradient to 1.
o.grad = 1;
// If we look up the derivative of `tanh(x)`, we know it's `1 - tanh(x)^2`.
n.grad = o.grad * (1 - o.data**2);
// Remember addition just passes the gradient along (the derivative of addition is 1).
x1w1.grad = n.grad * 1;
x2w2.grad = n.grad * 1;
b.grad = n.grad * 1;
// Remember that the derivative of `mul` is the other input.
x1.grad = x1w1.grad * w1.data;
x2.grad = x2w2.grad * w2.data;
w1.grad = x1w1.grad * x1.data;
w2.grad = x2w2.grad * x2.data;

print(await drawDot(o));
</textarea>

<p>Intuitively, it makes sense that the gradient of w2 is 0. If we wiggle w2, the
output won’t change because x2 is 0.</p>

<h2 id="automation">Automation</h2>

<p>Let’s automate this. For each operation, we’ll need to define the gradient with
respect to its inputs and multiply it by the output gradient.</p>

<p>We never want to call _backward for any node before we’ve done all its
dependencies, because we’ll need the result of the gradient for the deeper
nodes.</p>

<textarea disabled rows='22' data-src="utils.js">
export function getTopologicalOrder( startNode ) {
    const result = [];
    const visited = new Set();
    const visiting = new Set();

    function visit( node ) {
        if ( visited.has( node ) || ! node._prev ) return;
        if ( visiting.has( node ) ) {
            throw new Error("Cycle detected in computation graph.");
        }
        visiting.add( node );
        for ( const child of node._prev ) visit( child );
        visiting.delete( node );
        visited.add( node );
        result.push( node );
    }

    visit( startNode );

    return result;
}
</textarea>

<textarea disabled rows='92'>
class Value {
    static operations = new Map();
    constructor(_data, _children = [], _op) {
        this.data = _data;
        this._op = _op;
        this._prev = _children;
    }
    static addOperation(operation, forward) {
        this.operations.set(operation, forward);
        this.prototype[operation] = function(...args) {
            return new Value( null, [ this, ...args ], operation );
        }
    }
    forward() {
        const order = getTopologicalOrder(this);

        for (const node of order) {
            if (node._op) {
                const forward = Value.operations.get(node._op);
                const args = node._prev;
                const [data, ...grads] = forward(...args.map(arg => {
                    return arg instanceof Value ? arg.data : arg;
                }));
                node.data = data;
                node._backward = () => {
                    for (const [i, gradCalc] of grads.entries()) {
                        const grad = gradCalc(node.grad);
                        const child = args[i];
                        child.grad = grad;
                    }
                };
            }
        }
    }
    backward() {
        const reversed = getTopologicalOrder(this).reverse();

        for (const node of reversed) {
            node.grad = 0;
        }

        this.grad = 1;

        for (const node of reversed) {
            node._backward?.();
        }
    }
}

Value.addOperation('add', (a, b) => [
    a + b,
    (grad) => grad,
    (grad) => grad
]);
Value.addOperation('mul', (a, b) => [
    a * b,
    (grad) => b * grad,
    (grad) => a * grad
]);
Value.addOperation('tanh', (a) => {
    const tanh = Math.tanh(a);
    return [
        tanh,
        (grad) => (1 - tanh**2) * grad
    ]
});

// Inputs x1, x2.
const x1 = new Value(2);
const x2 = new Value(0);
// Weights w1, w2.
const w1 = new Value(-3);
const w2 = new Value(1);
// Bias b.
const b = new Value(6.8813735870195432);
// Output.
const x1w1 = x1.mul(w1);
const x2w2 = x2.mul(w2);
const n = x1w1.add(x2w2).add(b);
const o = n.tanh();

x1.label = 'x1';
x2.label = 'x2';
w1.label = 'w1';
w2.label = 'w2';
b.label = 'bias';

o.forward();
o.backward();

print(await drawDot(o));
</textarea>

<h2 id="fixing-the-accumulation-bug">Fixing the accumulation bug</h2>

<p>Let’s try adding the same value together.</p>

<textarea disabled rows='6'>
const a = new Value(3);
const b = a.add(a);
b.forward();
b.backward();
print(await drawDot(b));
</textarea>

<p>Note that the graph for <code class="language-plaintext highlighter-rouge">a</code> overlaps. But the gradient is 1, while it should be 2.</p>

<p>This is because we’re not accumulating the gradients. Instead, the backward pass
is overwritting the last calculated gradient. We need to fix the <code class="language-plaintext highlighter-rouge">backward</code> function to
accumulate the gradients.</p>

<textarea disabled rows='27'>
Value.prototype.forward = function() {
    const order = getTopologicalOrder(this);

    for (const node of order) {
        if (node._op) {
            const forward = Value.operations.get(node._op);
            const args = node._prev;
            const [data, ...grads] = forward(...args.map(arg => {
                return arg instanceof Value ? arg.data : arg;
            }));
            node.data = data;
            node._backward = () => {
                for (const [i, gradCalc] of grads.entries()) {
                    const grad = gradCalc(node.grad);
                    const child = args[i];
                    child.grad += grad;
                }
            };
        }
    }
}
const a = new Value(3);
const b = a.add(a);
b.forward();
b.backward();
print(await drawDot(b));
</textarea>

<h2 id="requires-grad">Requires grad</h2>

<p>We can add a <code class="language-plaintext highlighter-rouge">requiresGrad</code> flag to keep track of whether a value needs to
calculate a gradient or not.</p>

<h2 id="composite-tensor-operations">Composite tensor operations</h2>

<p>As previously explained, we can cluster operations together to form a composite
operation. We can also operate on matrices (or generally tensors) instead
of scaler values. We will take advantage of this because it is faster matrix
multiplication in one batch than to make all the atomic operations separately.</p>

<p>Let’s remember that for the next chapter.</p>

</article>
<script src="lib/acorn.min.js"></script>
<script src="common.js"></script>
<link rel="stylesheet" href="lib/codemirror.min.css" integrity="sha512-uf06llspW44/LZpHzHT6qBOIVODjWtv4MxCricRxkzvopAlSWnTf6hpZTFxuuZcuNE9CBQhqE0Seu1CoRk84nQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
<script src="lib/codemirror.min.js" integrity="sha512-8RnEqURPUc5aqFEN04aQEiPlSAdE0jlFS/9iGgUyNtwFnSKCXhmB6ZTNl7LnDtDWKabJIASzXrzD0K+LYexU9g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="lib/javascript.min.js" integrity="sha512-I6CdJdruzGtvDyvdO4YsiAq+pkWf2efgd1ZUSK2FnM/u2VuRASPC7GowWQrWyjxCZn6CT89s3ddGI+be0Ak9Fg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<style>
    .CodeMirror, .CodeMirror-scroll {
        height: auto;
        background: none;
    }
</style>
<script>
    document.querySelectorAll('textarea').forEach((textarea) => {
        textarea.editor = CodeMirror.fromTextArea(textarea, {
            mode: 'javascript',
            viewportMargin: Infinity,
            // theme: 'material',
            extraKeys: {
                'Shift-Enter': (cm) => {
                    textarea.button.focus();
                },
            },
        });
    })
</script>







  <a href="">Previous: 1. makemore: bigram</a>


<!-- Debug: 
    Original: /makemore-MLP
    After remove_first: makemore-MLP
    After relative_url: /makemore-MLP
  -->
  <a href="makemore-MLP">Next: 3. makemore: MLP</a>

<nav>
    <!-- <details> -->
        <!-- <summary>Table of contents</summary> -->
        <ul>
            
            <li><a href="">1. makemore: bigram</a></li>
            
            <li><a href="autograd">2. Autograd</a></li>
            
            <li><a href="makemore-MLP">3. makemore: MLP</a></li>
            
            <li><a href="makemore-learning-rate">3.1. makemore: Learning Rate</a></li>
            
            <li><a href="makemore-initialisation">3.2. makemore: Initialisation</a></li>
            
            <li><a href="makemore-batch-norm">3.3. makemore: Batch Norm</a></li>
            
            <li><a href="makemore-layer-organisation">3.4. makemore: Layer Organisation</a></li>
            
            <li><a href="diagnostic-tools">3.5. Diagnostic Tools</a></li>
            
            <li><a href="backprop-ninja">4. Backprop Ninja</a></li>
            
            <li><a href="makemore-wavenet">5. makemore: WaveNet</a></li>
            
            <li><a href="transformer">6. Transformer</a></li>
            
        </ul>
    <!-- </details> -->
</nav>
<script async src="lib/tex-mml-chtml.js"></script>
