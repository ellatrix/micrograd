<!doctype html>
<html lang="en">
<meta charset="utf-8">
<title>3.3. makemore: Batch Norm</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+3:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
<style>
    body {
        font-size: 1.4em;
        font-family: 'Source Sans 3', 'Hoefler Text', Georgia, 'Times New Roman', Times, serif;
        max-width: 900px;
        margin: 1em auto 10em;
    }

    :focus {
        outline-width: 2px;
        outline-style: solid;
        outline-color: #00f;
        border-radius: 2px;
        border-color: transparent;
    }

    [aria-label] {
        position: relative;
    }

    [aria-label]:focus-within::before {
        content: attr(aria-label);
        position: absolute;
        bottom: -20px;
        font-size: 12px;
        /* right: 0; */
    }

    input, button {
        font-family: inherit;
        font-size: inherit;
        font-size: 0.8em;
    }

    pre, code, samp, textarea {
        font-family: 'Source Code Pro', ui-monospace, Menlo, Monaco, "Cascadia Mono", "Segoe UI Mono", "Roboto Mono", "Oxygen Mono", "Ubuntu Monospace", "Source Code Pro", "Fira Mono",  "Droid Sans Mono", "Courier New", monospace !important;
        font-size: 0.8em !important;
        background: lightgoldenrodyellow;
    }

    pre, textarea {
        overflow: auto;
        padding: 1em;
    }

    pre[data-error] {
        background: lightpink;
    }

    details {
        margin: 1em 0;
    }

    aside {
        background-color: lavender;
        padding: .5em .7em;
    }

    textarea {
        width: 100%;
        border: none;
        resize: none;
        text-wrap: nowrap;
    }

    /* nav {
        position: fixed;
        top: 0;
        left: 0;
        bottom: 0;
        background: lightgoldenrodyellow;
        padding: 1em;
        overflow: auto;
    } */
</style>
<article>
<h1>3.3. makemore: Batch Norm</h1>

<aside>
    This covers the <a href="https://www.youtube.com/watch?v=TCH_1BHY58I">Building makemore Part 3: Activations &amp; Gradients, BatchNorm (40:40-1:03:07)</a> video.
</aside>

<p>In the previous section we saw that it was a good idea to have preactivation
values roughly unit gaussian (mean 0, standard deviation 1) at initialisation.
The insight of <a href="https://arxiv.org/pdf/1502.03167">Batch Normalization</a>, Sergey
Ioffe et al, 2015, was to simply make the preactivation values unit gaussian.
This is possible because it’s a perfectly differentiable operation.</p>

<p>Here is the formula for batch normalisation.</p>

<p>Mini batch mean:</p>

<div class="math">
$$
\mu_B \leftarrow \frac{1}{m} \sum_{i=1}^m x_i
$$
</div>

<p>Mini batch variance:</p>

<div class="math">
$$
\sigma_B^2 \leftarrow \frac{1}{m} \sum_{i=1}^m (x_i - \mu_B)^2
$$
</div>

<p>Normalise:</p>

<div class="math">
$$
\hat{x}_i \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$
</div>

<p>Scale and shift:</p>

<div class="math">
$$
x_i \leftarrow \gamma \hat{x}_i + \beta
$$
</div>

<p>Where \(\gamma\) and \(\beta\) are learnable parameters. \(\epsilon\) is a small constant
to avoid division by zero.</p>

<p>Why is gamma and beta needed? Well, we want it to be unit gaussian at
initialisation, but we also want to allow the neural net to change it.</p>

<p>Let’s implement it.</p>

<textarea disabled rows='12'>
import { random, softmaxByRow, matMul } from './1-bigram-utils.js';
import {
    Value,
    FloatMatrix,
    createFloatMatrix,
    buildDataSet,
    miniBatch,
    shuffle,
    createLossesGraph
} from './3-0-makemore-MLP-utils.js';
export { default as Plotly } from 'https://cdn.jsdelivr.net/npm/plotly.js-dist@2.26.2/+esm';
</textarea>

<textarea disabled rows='77' data-src="utils.js">
import { Value, FloatMatrix } from './3-0-makemore-MLP-utils.js';

Value.addOperation('batchNorm', (A, gain, bias) => {
    const n = A.shape.at(-1);
    const restDims = A.shape.slice(0, -1);
    const m = restDims.reduce((a, b) => a * b, 1);
    const bnraw = new FloatMatrix(A);
    const bnmean = createFloatMatrix( [n] );
    const bnvar = createFloatMatrix( [n] );
    const bnvarinv = createFloatMatrix( [n] );

    for (let n_ = n; n_--;) {
        let sum = 0;
        for (let m_ = m; m_--;) {
            sum += A[m_ * n + n_];
        }
        bnmean[n_] = sum / m;
    }

    for (let n_ = n; n_--;) {
        let variance = 0;
        for (let m_ = m; m_--;) {
            variance += (A[m_ * n + n_] - bnmean[n_]) ** 2;
        }
        // Apply Bessel's correction here
        bnvar[n_] = variance / (m - 0);
        bnvarinv[n_] = 1 / Math.sqrt(bnvar[n_] + 1e-5);
    }

    const bnout = createFloatMatrix( A.shape );

    for (let m_ = m; m_--;) {
        for (let n_ = n; n_--;) {
            const i = m_ * n + n_;
            bnraw[i] = (A[i] - bnmean[n_]) * bnvarinv[n_];
            bnout[i] = gain[n_] * bnraw[i] + bias[n_];
        }
    }

    return [
        bnout,
        (grad) => {
            // bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))
            const dA = new FloatMatrix(A);
            const outGradSum = createFloatMatrix( [n] );
            const outGradXbnrawSum = createFloatMatrix( [n] );
            const dGain = createFloatMatrix( gain.shape );
            const dBias = createFloatMatrix( bias.shape );

            // Calculate sums along the batch dimension (m)
            for (let n_ = n; n_--;) {
                for (let m_ = m; m_--;) {
                    const i = m_ * n + n_;
                    outGradSum[n_] += grad[i];
                    outGradXbnrawSum[n_] += grad[i] * bnraw[i];
                    dGain[n_] += grad[i] * bnraw[i];
                    dBias[n_] += grad[i];
                }
            }

            // Calculate the gradient
            for (let m_ = m; m_--;) {
                for (let n_ = n; n_--;) {
                    const i = m_ * n + n_;
                    dA[i] = gain[n_] * bnvarinv[n_] / m * (
                        m * grad[i] - 
                        outGradSum[n_] - 
                        m / (m - 0) * bnraw[i] * outGradXbnrawSum[n_]
                    );
                }
            }

            return [dA, dGain, dBias];
        },
    ];
});
</textarea>

<textarea disabled rows='48'>
function createNetwork() {
    const { embeddingDimensions, blockSize, neurons } = hyperParameters;
    const C = new Value( createFloatMatrix( [ vocabSize, embeddingDimensions ], random ) );
    const W1 = new Value( createFloatMatrix( [ embeddingDimensions * blockSize, neurons ], () => random() * 0.2 ) );
    // const b1 = new Value( createFloatMatrix( [ neurons ] ) );
    const W2 = new Value( createFloatMatrix( [ neurons, vocabSize ], () => random() * 0.01 ) );
    const b2 = new Value( createFloatMatrix( [ vocabSize ] ) );
    const bngain = new Value( createFloatMatrix( [ neurons ], () => 1 ) );
    const bnbias = new Value( createFloatMatrix( [ neurons ] ) );
    function logitFn( X ) {
        const embedding = C.gather( X ).reshape( [ X.shape[ 0 ], embeddingDimensions * blockSize ] );
        // Note: we should remove the bias here becaus with batchNorm it's no
        // longer doing anything.
        const preactivation = embedding.matMulBias( W1 );
        const hidden = preactivation.batchNorm( bngain, bnbias );
        const activation = hidden.tanh();
        return activation.matMulBias( W2, b2 );
    }
    logitFn.params = [ C, W1, /*b1,*/ W2, b2, bngain, bnbias ];
    return logitFn;
}

const response = await fetch('https://raw.githubusercontent.com/karpathy/makemore/master/names.txt');
const text = await response.text();
const names = text.split('\n');
const indexToCharMap = [ '.', ...new Set( names.join('') ) ].sort();
const stringToCharMap = {};

for ( let i = indexToCharMap.length; i--; ) {
    stringToCharMap[ indexToCharMap[ i ] ] = i;
}

const hyperParameters = {
    embeddingDimensions: 10,
    blockSize: 3,
    neurons: 200,
    batchSize: 32,
    learningRate: 0.1,
};

shuffle( names );
const n1 = Math.floor( names.length * 0.8 );
const n2 = Math.floor( names.length * 0.9 );
const [ Xtr, Ytr ] = buildDataSet( names.slice( 0, n1 ), stringToCharMap, hyperParameters.blockSize );
const [ Xdev, Ydev ] = buildDataSet( names.slice( n1, n2 ), stringToCharMap, hyperParameters.blockSize );
const [ Xte, Yte ] = buildDataSet( names.slice( n2 ), stringToCharMap, hyperParameters.blockSize );
const vocabSize = indexToCharMap.length;
</textarea>

<textarea disabled rows='4'>
const batchLosses = [];
const losses = [];
const network = createNetwork();
</textarea>

<textarea disabled rows='24'>
const graph = document.createElement( 'div' );
print(graph);
for ( let i = 0; i < 1000; i++ ) {
    const [ Xbatch, Ybatch ] = miniBatch( Xtr, Ytr, hyperParameters.batchSize );
    const loss = network( Xbatch ).softmaxCrossEntropy( Ybatch );
    await loss.forward();
    batchLosses.push( loss.data );
    await loss.backward();
    const learningRate = batchLosses.length < 2000 ? 0.1 : 0.01;
    for ( const param of network.params ) {
        for ( let i = param.data.length; i--; ) {
            param.data[ i ] -= learningRate * param.grad[ i ];
        }
    }

    if ( batchLosses.length % 100 === 0 ) {
        const loss = network( Xdev ).softmaxCrossEntropy( Ydev );
        await loss.forward();
        losses.push( loss.data );
    }

    await createLossesGraph( graph, batchLosses, losses );
}
</textarea>

<p>We don’t expect much improvement because it’s a very simple network. But once
the network becomes deeper with different operation, it will become very
difficult to manually tune it so that all the activation are roughly gaussian.
It also has a regularising effect.</p>

<p>This comes at a terrible cost. The examples in the batch are coupled.</p>

<p>Now we need to find a way to sample from the network. The neural net expects
batches as an input now. We need to keep track of the batch mean and standard
deviaton over time.</p>

<p>Either we take the mean and standard deviation of the entire dataset, or we keep
a running mean and standard deviation. If we do the former, we need to refactor
the batchNorm operation to return the mean and standard deviation (or take it as
an input and calculate it outside) so that we can update the running mean and
standard deviation on every batch iteration.</p>

<p>To do that, let’s first organise our layers better.</p>

<p>Note: biases in preactivation are now no longer doing anything, so we should
removed them to not waste compute. The batch norm bias is now in charge of
biasing.</p>

<p>Normally we place batch norm after layers that have multiplication.</p>

<p>Group normalisation and others have become more common because they avoid the
coupling of examples, which is less bug prone. It’s better to avoid batch norm
if we can.</p>

</article>
<script src="lib/acorn.min.js"></script>
<script src="common.js"></script>
<link rel="stylesheet" href="lib/codemirror.min.css" integrity="sha512-uf06llspW44/LZpHzHT6qBOIVODjWtv4MxCricRxkzvopAlSWnTf6hpZTFxuuZcuNE9CBQhqE0Seu1CoRk84nQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
<script src="lib/codemirror.min.js" integrity="sha512-8RnEqURPUc5aqFEN04aQEiPlSAdE0jlFS/9iGgUyNtwFnSKCXhmB6ZTNl7LnDtDWKabJIASzXrzD0K+LYexU9g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="lib/javascript.min.js" integrity="sha512-I6CdJdruzGtvDyvdO4YsiAq+pkWf2efgd1ZUSK2FnM/u2VuRASPC7GowWQrWyjxCZn6CT89s3ddGI+be0Ak9Fg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<style>
    .CodeMirror, .CodeMirror-scroll {
        height: auto;
        background: none;
    }
</style>
<script>
    document.querySelectorAll('textarea').forEach((textarea) => {
        textarea.editor = CodeMirror.fromTextArea(textarea, {
            mode: 'javascript',
            viewportMargin: Infinity,
            // theme: 'material',
            extraKeys: {
                'Shift-Enter': (cm) => {
                    textarea.button.focus();
                },
            },
        });
    })
</script>







  <a href="makemore-initialisation">Previous: 3.2. makemore: Initialisation</a>


<!-- Debug: 
    Original: /makemore-layer-organisation
    After remove_first: makemore-layer-organisation
    After relative_url: /makemore-layer-organisation
  -->
  <a href="makemore-layer-organisation">Next: 3.4. makemore: Layer Organisation</a>

<nav>
    <!-- <details> -->
        <!-- <summary>Table of contents</summary> -->
        <ul>
            
            <li><a href="">1. makemore: bigram</a></li>
            
            <li><a href="autograd">2. Autograd</a></li>
            
            <li><a href="makemore-MLP">3. makemore: MLP</a></li>
            
            <li><a href="makemore-learning-rate">3.1. makemore: Learning Rate</a></li>
            
            <li><a href="makemore-initialisation">3.2. makemore: Initialisation</a></li>
            
            <li><a href="makemore-batch-norm">3.3. makemore: Batch Norm</a></li>
            
            <li><a href="makemore-layer-organisation">3.4. makemore: Layer Organisation</a></li>
            
            <li><a href="diagnostic-tools">3.5. Diagnostic Tools</a></li>
            
            <li><a href="backprop-ninja">4. Backprop Ninja</a></li>
            
            <li><a href="makemore-wavenet">5. makemore: WaveNet</a></li>
            
            <li><a href="transformer">6. Transformer</a></li>
            
        </ul>
    <!-- </details> -->
</nav>
<script async src="lib/tex-mml-chtml.js"></script>
