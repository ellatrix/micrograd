<!doctype html>
<html lang="en">
<meta charset="utf-8">
<title>3.2. makemore: Initialisation</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+3:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
<style>
    body {
        font-size: 1.4em;
        font-family: 'Source Sans 3', 'Hoefler Text', Georgia, 'Times New Roman', Times, serif;
        max-width: 900px;
        margin: 1em auto 10em;
    }

    :focus {
        outline-width: 2px;
        outline-style: solid;
        outline-color: #00f;
        border-radius: 2px;
        border-color: transparent;
    }

    [aria-label] {
        position: relative;
    }

    [aria-label]:focus-within::before {
        content: attr(aria-label);
        position: absolute;
        bottom: -20px;
        font-size: 12px;
        /* right: 0; */
    }

    input, button {
        font-family: inherit;
        font-size: inherit;
        font-size: 0.8em;
    }

    pre, code, samp, textarea {
        font-family: 'Source Code Pro', ui-monospace, Menlo, Monaco, "Cascadia Mono", "Segoe UI Mono", "Roboto Mono", "Oxygen Mono", "Ubuntu Monospace", "Source Code Pro", "Fira Mono",  "Droid Sans Mono", "Courier New", monospace !important;
        font-size: 0.8em !important;
        background: lightgoldenrodyellow;
    }

    pre, textarea {
        overflow: auto;
        padding: 1em;
    }

    pre[data-error] {
        background: lightpink;
    }

    details {
        margin: 1em 0;
    }

    aside {
        background-color: lavender;
        padding: .5em .7em;
    }

    textarea {
        width: 100%;
        border: none;
        resize: none;
        text-wrap: nowrap;
    }

    /* nav {
        position: fixed;
        top: 0;
        left: 0;
        bottom: 0;
        background: lightgoldenrodyellow;
        padding: 1em;
        overflow: auto;
    } */
</style>
<article>
<h1>3.2. makemore: Initialisation</h1>

<textarea disabled rows='12'>
import { random, softmaxByRow, matMul } from './1-bigram-utils.js';
import {
    Value,
    FloatMatrix,
    IntMatrix,
    buildDataSet,
    miniBatch,
    shuffle,
    createLossesGraph
} from './3-0-makemore-MLP-utils.js';
import Plotly from './lib/plotly.js';
</textarea>

<textarea disabled rows='45'>
const response = await fetch('lib/names.txt');
const text = await response.text();
const names = text.split('\n');
const indexToCharMap = [ '.', ...new Set( names.join('') ) ].sort();
const stringToCharMap = {};

for ( let i = indexToCharMap.length; i--; ) {
    stringToCharMap[ indexToCharMap[ i ] ] = i;
}

const hyperParameters = {
    embeddingDimensions: 10,
    blockSize: 3,
    neurons: 200,
    batchSize: 32,
    learningRate: 0.1,
};

shuffle( names );
const n1 = Math.floor( names.length * 0.8 );
const n2 = Math.floor( names.length * 0.9 );
const [ Xtr, Ytr ] = buildDataSet( names.slice( 0, n1 ), stringToCharMap, hyperParameters.blockSize );
const [ Xdev, Ydev ] = buildDataSet( names.slice( n1, n2 ), stringToCharMap, hyperParameters.blockSize );
const [ Xte, Yte ] = buildDataSet( names.slice( n2 ), stringToCharMap, hyperParameters.blockSize );
const vocabSize = indexToCharMap.length;

function createNetwork() {
    const { embeddingDimensions, blockSize, neurons } = hyperParameters;
    const C = new Value( new FloatMatrix( random, [ vocabSize, embeddingDimensions ] ) );
    const W1 = new Value( new FloatMatrix( random, [ embeddingDimensions * blockSize, neurons ] ) );
    const b1 = new Value( new FloatMatrix( random, [ neurons ] ) );
    const W2 = new Value( new FloatMatrix( random, [ neurons, vocabSize ] ) );
    const b2 = new Value( new FloatMatrix( random, [ vocabSize ] ) );
    function logitFn( X ) {
        const embedding = C.gather( X ).reshape( [ X.shape[ 0 ], embeddingDimensions * blockSize ] );
        const hidden = embedding.matMulBias( W1, b1 ).tanh();
        return hidden.matMulBias( W2, b2 );
    }
    logitFn.params = [ C, W1, b1, W2, b2 ];
    return logitFn;
}
const batchLosses = [];
const losses = [];
const network = createNetwork();
</textarea>

<textarea disabled rows='24'>
const graph = document.createElement( 'div' );
print(graph);
for ( let i = 0; i < 200; i++ ) {
    const [ Xbatch, Ybatch ] = miniBatch( Xtr, Ytr, hyperParameters.batchSize );
    const loss = network( Xbatch ).softmaxCrossEntropy( Ybatch );
    await loss.forward();
    batchLosses.push( loss.data );
    await loss.backward();
    const learningRate = batchLosses.length < 2000 ? 0.1 : 0.01;
    for ( const param of network.params ) {
        for ( let i = param.data.length; i--; ) {
            param.data[ i ] -= learningRate * param.grad[ i ];
        }
    }

    if ( batchLosses.length % 100 === 0 ) {
        const loss = network( Xdev ).softmaxCrossEntropy( Ydev );
        await loss.forward();
        losses.push( loss.data );
    }

    await createLossesGraph( graph, batchLosses, losses );
}
</textarea>

<p>The network is very unproperly configured at initialization. What initial loss
would we expect? The probability of any one character should be roughly 1/vocabSize. It should be a uniform distribution. So the loss we would expect is -log(1/vocabSize).</p>

<textarea disabled rows='2'>
print( -Math.log( 1 / 27 ) );
</textarea>

<p>Yet the inital loss is much higher.</p>

<p>Let’s say we have a smaller network where 4 logits come out.</p>

<textarea disabled rows='3'>
const logits = new FloatMatrix( [ 0, 0, 0, 0 ], [ 1, 4 ] );
print( softmaxByRow( logits ) );
</textarea>

<p>The softmax of these logits gives us a probability distribution, and we can see
that it is exactly uniform if all logits are the same.</p>

<p>Let’s initialize a new network and check the values of the logits. Perhaps run
it a few to get some variation.</p>

<textarea disabled rows='4'>
const logits = createNetwork()( Xdev );
await logits.forward();
print( logits.data );
</textarea>

<p>As you can see the logits can take on large values and be quite spread out, 
which drives up the loss.</p>

<p>So how can we decrease it? The last step of the network is a matMul, so we
should make sure the weights are closer to zero. We can initialize the bias with
zero.</p>

<textarea disabled rows='16'>
function createNetwork() {
    const { embeddingDimensions, blockSize, neurons } = hyperParameters;
    const C = new Value( new FloatMatrix( random, [ vocabSize, embeddingDimensions ] ) );
    const W1 = new Value( new FloatMatrix( random, [ embeddingDimensions * blockSize, neurons ] ) );
    const b1 = new Value( new FloatMatrix( random, [ neurons ] ) );
    const W2 = new Value( new FloatMatrix( () => random() * 0.01, [ neurons, vocabSize ] ) );
    const b2 = new Value( new FloatMatrix( null, [ vocabSize ] ) );
    function logitFn( X ) {
        const embedding = C.gather( X ).reshape( [ X.shape[ 0 ], embeddingDimensions * blockSize ] );
        const hidden = embedding.matMulBias( W1, b1 ).tanh();
        return hidden.matMulBias( W2, b2 );
    }
    logitFn.params = [ C, W1, b1, W2, b2 ];
    return logitFn;
}
</textarea>

<p>Let’s run it again.</p>

<textarea disabled rows='4'>
const logits = createNetwork()( Xdev );
await logits.forward();
print( logits.data );
</textarea>

<p>Let’s check the loss.</p>

<textarea disabled rows='4'>
const loss = createNetwork()( Xdev ).softmaxCrossEntropy( Ydev );
await loss.forward();
print( loss.data );
</textarea>

<p>This is very close to what we’re expecting!</p>

<p>Can we actually set the weights to zero as well? Then we’d get exactly what
we’re looking for. But we need some entropy in the weights to break the symmetry
and allow for learning.</p>

<textarea disabled rows='4'>
const batchLosses = [];
const losses = [];
const network = createNetwork();
</textarea>

<textarea disabled rows='24'>
const graph = document.createElement( 'div' );
print(graph);
for ( let i = 0; i < 200; i++ ) {
    const [ Xbatch, Ybatch ] = miniBatch( Xtr, Ytr, hyperParameters.batchSize );
    const loss = network( Xbatch ).softmaxCrossEntropy( Ybatch );
    await loss.forward();
    batchLosses.push( loss.data );
    await loss.backward();
    const learningRate = batchLosses.length < 2000 ? 0.1 : 0.01;
    for ( const param of network.params ) {
        for ( let i = param.data.length; i--; ) {
            param.data[ i ] -= learningRate * param.grad[ i ];
        }
    }

    if ( batchLosses.length % 100 === 0 ) {
        const loss = network( Xdev ).softmaxCrossEntropy( Ydev );
        await loss.forward();
        losses.push( loss.data );
    }

    await createLossesGraph( graph, batchLosses, losses );
}
</textarea>

<p>Great, now we have much less of a hockey stick graph, and waste less training
time.</p>

<p>But there’s still a deeper issue at initialization. The logits are now ok, but
the problem now is with the output of the hidden layer. Let’s have a look at the
histogram.</p>

<textarea disabled rows='23'>
const [ X ] = miniBatch( Xtr, Ytr, hyperParameters.batchSize );
const { embeddingDimensions, blockSize, neurons } = hyperParameters;
const C = new Value( new FloatMatrix( random, [ vocabSize, embeddingDimensions ] ) );
const W1 = new Value( new FloatMatrix( random, [ embeddingDimensions * blockSize, neurons ] ) );
const b1 = new Value( new FloatMatrix( random, [ neurons ] ) );
const embedding = C.gather( X ).reshape( [ X.shape[ 0 ], embeddingDimensions * blockSize ] );
const hidden = embedding.matMulBias( W1, b1 ).tanh();
await hidden.forward();
print( await Plotly.newPlot( document.createElement('div'), [ { x: Array.from( hidden.data ), type: 'histogram' } ] ) );
print( await Plotly.newPlot( document.createElement('div'), [{
    z: [...Array(hidden.data.shape[0])].map((_, i) => 
        Array.from(hidden.data).slice(i * hidden.data.shape[1], (i + 1) * hidden.data.shape[1])
        .map(value => value > 0.9 ? 1 : 0)
    ),
    type: 'heatmap',
    colorscale: 'Greys',
    showscale: false
    }], {
    height: 300,
    xaxis: { visible: false },
    yaxis: { visible: false }
},{ displayModeBar: false }) );
</textarea>

<p>As you can see, a very large number come out close to 1 or -1. The tanh is very
active (squashing large values), and the output is not uniform.</p>

<p>Why is this a problem? You may recall the the derivative of tanh is 1 - tanh^2.
When the values are 1 or -1, the derivative is 0, we are killing the gradient.</p>

<p>The solution is the same as before: in front of the the tanh activation, we have
a matMul, so if we scale the weights down, the preactivations will be closer to zero,
and the tanh will squash less because there’s no extreme values.</p>

<textarea disabled rows='20'>
const W1 = new Value( new FloatMatrix( () => random() * 0.2, [ embeddingDimensions * blockSize, neurons ] ) );
const b1 = new Value( new FloatMatrix( null, [ neurons ] ) );
const embedding = C.gather( X ).reshape( [ X.shape[ 0 ], embeddingDimensions * blockSize ] );
const hidden = embedding.matMulBias( W1, b1 ).tanh();
await hidden.forward();
print( await Plotly.newPlot( document.createElement('div'), [ { x: Array.from( hidden.data ), type: 'histogram' } ] ) );
print( await Plotly.newPlot( document.createElement('div'), [{
    z: [...Array(hidden.data.shape[0])].map((_, i) => 
        Array.from(hidden.data).slice(i * hidden.data.shape[1], (i + 1) * hidden.data.shape[1])
        .map(value => value > 0.9 ? 1 : 0)
    ),
    type: 'heatmap',
    colorscale: 'Greys',
    showscale: false
    }], {
    height: 300,
    xaxis: { visible: false },
    yaxis: { visible: false }
},{ displayModeBar: false }) );
</textarea>

<p>Let’s put it all together.</p>

<textarea disabled rows='16'>
function createNetwork() {
    const { embeddingDimensions, blockSize, neurons } = hyperParameters;
    const C = new Value( new FloatMatrix( random, [ vocabSize, embeddingDimensions ] ) );
    const W1 = new Value( new FloatMatrix( () => random() * 0.2, [ embeddingDimensions * blockSize, neurons ] ) );
    const b1 = new Value( new FloatMatrix( null, [ neurons ] ) );
    const W2 = new Value( new FloatMatrix( () => random() * 0.01, [ neurons, vocabSize ] ) );
    const b2 = new Value( new FloatMatrix( null, [ vocabSize ] ) );
    function logitFn( X ) {
        const embedding = C.gather( X ).reshape( [ X.shape[ 0 ], embeddingDimensions * blockSize ] );
        const hidden = embedding.matMulBias( W1, b1 ).tanh();
        return hidden.matMulBias( W2, b2 );
    }
    logitFn.params = [ C, W1, b1, W2, b2 ];
    return logitFn;
}
</textarea>

<textarea disabled rows='4'>
const batchLosses = [];
const losses = [];
const network = createNetwork();
</textarea>

<textarea disabled rows='24'>
const graph = document.createElement( 'div' );
print(graph);
for ( let i = 0; i < 200; i++ ) {
    const [ Xbatch, Ybatch ] = miniBatch( Xtr, Ytr, hyperParameters.batchSize );
    const loss = network( Xbatch ).softmaxCrossEntropy( Ybatch );
    await loss.forward();
    batchLosses.push( loss.data );
    await loss.backward();
    const learningRate = batchLosses.length < 2000 ? 0.1 : 0.01;
    for ( const param of network.params ) {
        for ( let i = param.data.length; i--; ) {
            param.data[ i ] -= learningRate * param.grad[ i ];
        }
    }

    if ( batchLosses.length % 100 === 0 ) {
        const loss = network( Xdev ).softmaxCrossEntropy( Ydev );
        await loss.forward();
        losses.push( loss.data );
    }

    await createLossesGraph( graph, batchLosses, losses );
}
</textarea>

<p>The difference is not very big, but the deeper the network is, the less
forgiving it is to these errors.</p>

<p>How do we know with what value to scale the weights with?</p>

<p>Let’s say we random inputs and random weights, both of which would be
initialized at a standard deviation of ~1.</p>

<textarea disabled rows='14'>
function standardDeviation(values) {
    const mean = values.reduce((a, b) => a + b) / values.length;
    const variance = values
        .map(x => Math.pow(x - mean, 2))
        .reduce((a, b) => a + b) / values.length;
    return Math.sqrt(variance);
}

const X = new FloatMatrix( random, [ 1000, 10 ] );
const W = new FloatMatrix( random, [ 10, 200 ] );
const Y = matMul( X, W );
print( standardDeviation( Array.from( X ) ) );
print( standardDeviation( Array.from( Y ) ) );
</textarea>

<p>So how do we scale the weights to preserve the distribution? If we scale them
up, there will be more and more extreme values. If we scale them down, the
standart deviation will shrink. What do we multiply the weights by to exactly
preserve it?</p>

<p>It turns out the correct mathematical answer is to multiply the weights by
1/sqrt( weight number of rows ).</p>

<textarea disabled rows='4'>
const W = new FloatMatrix( () => random() / 10**0.5, [ 10, 200 ] );
const Y = matMul( X, W );
print( standardDeviation( Array.from( Y ) ) );
</textarea>

<p>As you can see, the standard deviation is now ~1 too.</p>

<p>But on top of this we also have the tanh activation, which squashes the values
further.</p>

<textarea disabled rows='7'>
const W = new FloatMatrix( () => random() / 10**0.5, [ 10, 200 ] );
const Y = matMul( X, W );
for ( let i = Y.length; i--; ) {
    Y[ i ] = Math.tanh( Y[ i ] );
}
print( standardDeviation( Array.from( Y ) ) );
</textarea>

<p>So we’ll need a slight gain to compensate for this. It turns out that, for tanh,
a gain of 5/3 is needed. Other activation functions may need a different gain.
E.g. relu, which throws away 50% of the values, needs a gain of 2. See Kaiming
He, 2020. This would be called the Kaiming initialization, which we can use
instead of the random scale.</p>

<textarea disabled rows='2'>
print( (5/3) / (hyperParameters.embeddingDimensions * hyperParameters.blockSize**0.5) );
</textarea>

<p>Looks like the correct values would be 0.1 instead of 0.2.</p>

<textarea disabled rows='21'>
const [ X ] = miniBatch( Xtr, Ytr, hyperParameters.batchSize );
const W1 = new Value( new FloatMatrix( () => random() * 0.1, [ embeddingDimensions * blockSize, neurons ] ) );
const b1 = new Value( new FloatMatrix( null, [ neurons ] ) );
const embedding = C.gather( X ).reshape( [ X.shape[ 0 ], embeddingDimensions * blockSize ] );
const hidden = embedding.matMulBias( W1, b1 ).tanh();
await hidden.forward();
print( await Plotly.newPlot( document.createElement('div'), [ { x: Array.from( hidden.data ), type: 'histogram' } ] ) );
print( await Plotly.newPlot( document.createElement('div'), [{
    z: [...Array(hidden.data.shape[0])].map((_, i) => 
        Array.from(hidden.data).slice(i * hidden.data.shape[1], (i + 1) * hidden.data.shape[1])
        .map(value => value > 0.9 ? 1 : 0)
    ),
    type: 'heatmap',
    colorscale: 'Greys',
    showscale: false
    }], {
    height: 300,
    xaxis: { visible: false },
    yaxis: { visible: false }
},{ displayModeBar: false }) );
</textarea>

<p>Indeed, if we fill in 0.1, the output has a standard deviation of one, and very
little numbers are -1 or 1.</p>

<p>Let’s look at a more modern solution, called batch normalization, which removes
the need for such careful initialization.</p>

</article>
<script src="lib/acorn.min.js"></script>
<script src="common.js"></script>
<link rel="stylesheet" href="lib/codemirror.min.css" integrity="sha512-uf06llspW44/LZpHzHT6qBOIVODjWtv4MxCricRxkzvopAlSWnTf6hpZTFxuuZcuNE9CBQhqE0Seu1CoRk84nQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
<script src="lib/codemirror.min.js" integrity="sha512-8RnEqURPUc5aqFEN04aQEiPlSAdE0jlFS/9iGgUyNtwFnSKCXhmB6ZTNl7LnDtDWKabJIASzXrzD0K+LYexU9g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="lib/javascript.min.js" integrity="sha512-I6CdJdruzGtvDyvdO4YsiAq+pkWf2efgd1ZUSK2FnM/u2VuRASPC7GowWQrWyjxCZn6CT89s3ddGI+be0Ak9Fg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<style>
    .CodeMirror, .CodeMirror-scroll {
        height: auto;
        background: none;
    }
</style>
<script>
    document.querySelectorAll('textarea').forEach((textarea) => {
        textarea.editor = CodeMirror.fromTextArea(textarea, {
            mode: 'javascript',
            viewportMargin: Infinity,
            // theme: 'material',
            extraKeys: {
                'Shift-Enter': (cm) => {
                    textarea.button.focus();
                },
            },
        });
    })
</script>







  <a href="makemore-learning-rate">Previous: 3.1. makemore: Learning Rate</a>


<!-- Debug: 
    Original: /makemore-batch-norm
    After remove_first: makemore-batch-norm
    After relative_url: /makemore-batch-norm
  -->
  <a href="makemore-batch-norm">Next: 3.3. makemore: Batch Norm</a>

<nav>
    <!-- <details> -->
        <!-- <summary>Table of contents</summary> -->
        <ul>
            
            <li><a href="">1. makemore: bigram</a></li>
            
            <li><a href="autograd">2. Autograd</a></li>
            
            <li><a href="makemore-MLP">3. makemore: MLP</a></li>
            
            <li><a href="makemore-learning-rate">3.1. makemore: Learning Rate</a></li>
            
            <li><a href="makemore-initialisation">3.2. makemore: Initialisation</a></li>
            
            <li><a href="makemore-batch-norm">3.3. makemore: Batch Norm</a></li>
            
            <li><a href="makemore-layer-organisation">3.4. makemore: Layer Organisation</a></li>
            
            <li><a href="makemore-wave-net">5. makemore: Wave Net</a></li>
            
        </ul>
    <!-- </details> -->
</nav>
<script async src="lib/tex-mml-chtml.js"></script>
