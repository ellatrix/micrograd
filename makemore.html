<p>Click <em>Sample</em>, and you'll see the output is very random, because the
neural network hasn't trained yet. Now click <em>Run</em> to start training (<a
href="https://raw.githubusercontent.com/karpathy/makemore/master/names.txt">data</a>).
When the loss is under 3, click <em>Sample</em> again, and you'll see the output
has gotten a bit better. Try to reach a loss of 2.7 or lower, but it's possible
to get it under 2.5! Note that the neural network tries to predict only based on
the previous character, so it won't get very good!</p>
<p>Excluding the table rendering this neural network is 300 lines of code and
has zero dependencies.</p>
<input type="text" id="learningRateInput" value="1">
<button id="runButton">Run</button>
<code>loss: </code><samp id="lossOutput">x.xxxx</samp>;
<code>iterations: </code><samp id="iterationsOutput">x</samp><hr>
<button id="sampleButton">Sample</button>
<samp id="sampleOutput"></samp>
<hr>
<p>Each row contains the probabilities for the character in the column to be the
next character. For example, the first row shows the probabilities for the
starting character. Each row sums to 1.</p>
<table id="table"></table>
<script src="matmul-cpu.js"></script>
<script src="matmul-gpu.js"></script>
<script src="matrix.js"></script>
<script src="table.js"></script>
<script>
    ( async () => {
        const res = await fetch('https://raw.githubusercontent.com/karpathy/makemore/master/names.txt');
        const text = await res.text();
        Value.gpu = await GPU();

        const indexToCharMap = [ '.', ...new Set( text ) ].sort().slice( 1 );
        const totalChars = indexToCharMap.length;
        const stringToCharMap = {};

        for ( let i = totalChars; i--; ) {
            stringToCharMap[ indexToCharMap[ i ] ] = i;
        }

        const names = text.split('\n');

        // Inputs.
        const xs = [];
        // Targets, or labels.
        const ys = [];

        for ( const name of names ) {
            const exploded = '.' + name + '.';
            i = 1;
            while ( exploded[ i ] ) {
                const bigram = exploded[i - 1] + exploded[i];
                const indexOfChar1 = stringToCharMap[ exploded[ i - 1 ] ];
                const indexOfChar2 = stringToCharMap[ exploded[ i ] ];
                xs.push( indexOfChar1 );
                ys.push( indexOfChar2 );
                i++;
            }
        }

        const neurons = totalChars;

        // One hidden layer.
        const W = new Variable( random( [ totalChars, neurons ] ) );

        const iterations = 200;

        let totalIterations = 0;
        let lastLoss = Infinity;
        let running = false;

        makeTable( W.data, indexToCharMap );

        function logitFn( X ) {
            return new Value( oneHot( X, totalChars ) ).matMul( W );
        }

        function lossFn( X, Y ) {
            return logitFn( X ).softmaxCrossEntropy( oneHot( Y, totalChars ) );
        }

        runButton.onclick = () => {
            if ( running ) {
                running = false;
                runButton.textContent = 'Run';
                learningRateInput.disabled = false;
                return;
            }

            running = true;
            runButton.textContent = 'Stop';
            learningRateInput.disabled = true;

            const learningRate = parseFloat( learningRateInput.value ) || 0.1;
            const singleRun = async () => {
                const loss = lossFn( xs, ys );
                await loss.forward();
                const lossValue = loss.data[0];
                const action = lossValue < lastLoss ? 'log' : 'error';
                console[action](`Loss after iteration ${i}: ${lossValue}`);
                lastLoss = lossValue;
                if ( action === 'error' && i > 5 ) {
                    learningRateInput.value = learningRate / 10;
                    running = false;
                    runButton.textContent = 'Run';
                    learningRateInput.disabled = false;
                    alert( `Loss increased. Try a lower learning rate. Suggested ${ learningRate / 10 }. Click "Run" to continue with the new rate.` );
                    return;
                }
                await loss.backward();
                for ( let i = W.data.length; i--; ) W.data[ i ] -= learningRate * W.grad[ i ];
                lossOutput.innerText = lossValue.toFixed( 4 );
                totalIterations++;
                iterationsOutput.innerText = totalIterations;
                makeTable( W.data, indexToCharMap );
            }

            let i = 0;

            const callback = async () => {
                if ( running && i++ < iterations ) {
                    await singleRun();
                    requestAnimationFrame( callback );
                } else {
                    running = false;
                    runButton.textContent = 'Run';
                    learningRateInput.disabled = false;
                }
            }

            requestAnimationFrame( callback );
        }

        sampleButton.onclick = async () => {
            const names = [];

            for (let i = 0; i < 5; i++) {
                const indices = [ 0 ];

                do {
                    const context = indices.slice( -1 );
                    const logits = await logitFn( context ).forward();
                    const probs = softmaxByRow( logits );
                    indices.push( sample( probs ) );
                } while ( indices[ indices.length - 1 ] );

                names.push( indices.slice( 1, -1 ).map( ( i ) => indexToCharMap[ i ] ).join( '' ) );
            }

            sampleOutput.innerText = names.join( ', ' );
        }
    })();
</script>