<meta charset="utf-8">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+3:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
<style>
    body {
        font-size: 1.4em;
        font-family: 'Source Sans 3', 'Hoefler Text', Georgia, 'Times New Roman', Times, serif;
        max-width: 900px;
        margin: 1em auto 10em;
    }

    :focus {
        outline-width: 2px;
        outline-style: solid;
        outline-color: #00f;
        border-radius: 2px;
        border-color: transparent;
    }

    [aria-label] {
        position: relative;
    }

    [aria-label]:focus-within::before {
        content: attr(aria-label);
        position: absolute;
        bottom: -20px;
        font-size: 12px;
        /* right: 0; */
    }

    input, button {
        font-family: inherit;
        font-size: inherit;
        font-size: 0.8em;
    }

    pre, code, samp {
        font-family: 'Source Code Pro', ui-monospace, Menlo, Monaco, "Cascadia Mono", "Segoe UI Mono", "Roboto Mono", "Oxygen Mono", "Ubuntu Monospace", "Source Code Pro", "Fira Mono",  "Droid Sans Mono", "Courier New", monospace !important;
        font-size: 0.8em !important;
    }

    pre {
        overflow: auto;
        background: lightgoldenrodyellow;
        padding: 1em;
    }

    pre[data-error] {
        background: lightpink;
    }

    details {
        margin: 1em 0;
    }

    aside {
        background-color: lavender;
        padding: .5em .7em;
    }
</style>
<h1>
    Building a neural network from scratch in JavaScript, part 1
</h1>
<aside>
    This is an interactive notebook. You can edit the snippets and use
    <code>print( var, label )</code>.
</aside>
<!-- <p>If you just want to check out the demo, click <button>Run all</button>. Please run in Chrome!</p> -->
<p>
    I really enjoyed Andrej Karpathy’s “Zero to Hero” series because in it,
    he builds everything from scratch. Or almost at least. This post follows his
    second video lecture, but in JS instead. While he uses PyTorch’s low level
    APIs, we’ll be creating absolutely everything from scratch, without any
    fancy machine learning frameworks!
</p>
<p>
    Let’s begin with a simple neural network that learns which character is most
    likely to follow given a character. A simple character-level bigram model.
    We will train on names, and when done we can generate more (“makemore”)
    name-like sequences.
</p>
<p>
    First, we need to fetch a list of names and process those names into a
    useable dataset.
</p>
<script src>
    const response = await fetch('https://raw.githubusercontent.com/karpathy/makemore/master/names.txt');
    export const text = await response.text();
    export const names = text.split('\n');
</script>
<p>
    Now that we have an array of names, let’s create index-to-character and
    character-to-index mappings. Neural networks cannot operate on characters,
    we’ll need to map the characters to numbers. Additionally we’ll want to
    include a character that can be used to signify the start or end of a name.
    Let’s use a newline, set at index 0.
</p>
<script src>
    export const indexToCharMap = [ '.', ...new Set( names.join('') ) ].sort();
    export const stringToCharMap = {};

    for ( let i = indexToCharMap.length; i--; ) {
        stringToCharMap[ indexToCharMap[ i ] ] = i;
    }
</script>
<p>
    Next, let’s create the training dataset. We need pairs (bigrams) of a
    character x preceding a character y. My name <code>ella</code> has 5
    examples: <code>.e</code>, <code>el</code>, <code>ll</code>,
    <code>la</code>, and <code>a.</code>! To do this we prepend and append the
    names with <code>.</code>
</p>
<script src>
    export const xs = []; // Inputs.
    export const ys = []; // Targets, or labels.

    for ( const name of names ) {
        const exploded = '.' + name + '.';
        let i = 1;
        while ( exploded[ i ] ) {
            xs.push( stringToCharMap[ exploded[ i - 1 ] ] );
            ys.push( stringToCharMap[ exploded[ i ] ] );
            i++;
        }
    }
</script>
<p>
    That’s it! Now we have our training data. For example for <code>ella</code>,
    <code>x[0]</code> gives us <code>0</code> (starting character) and
    <code>y[0]</code> gives us <code>5</code> (E). <code>x[1]</code> gives us
    <code>5</code> (E) and <code>y[1]</code> gives us <code>12</code> (L) and so
    on.
</p>
<p>
    We now want to build a neural network with a single layer, in which the
    weights are a 27×27 matrix. You can see it as a table containing the
    probabilities of a character in each column being followed by the character
    in the row. We could enlarge the number of columns, but this is easier to
    visualise later.
</p>
<p>
    The weights are trainable and should be initialised randomly to break
    symmetry, otherwise all gradients will be the same. JavaScript does not have
    a native matrix data type, so let’s use flat arrays with a
    <code>shape</code> property. For example a <code>shape</code> of <code>[ 20,
    27 ]</code> is a matrix with 20 rows and 27 columns. Let’s create a utility
    function to create a matrix.
</p>
<script src>
    export class FloatMatrix extends Float32Array {
        constructor( data, shape = data?.shape || [] ) {
            const length = shape.reduce( ( a, b ) => a * b, 1 );

            super( data || length );

            if ( this.length !== length ) {
                throw new Error( 'Shape does not match data length.' );
            }

            this.shape = shape;
        }
    }

    print( new FloatMatrix( null, [ 2, 3 ] ) );
</script>
<p>
    With <code>FloatMatrix</code>, we can now initialise our weights matrix more
    easily. Let’s add random values between 1 and -1.
</p>
<script src>
    const totalChars = indexToCharMap.length;
    export const W = new FloatMatrix( null, [ totalChars, totalChars ] );
    for ( let i = W.length; i--; ) W[ i ] = Math.random() * 2 - 1;
</script>
<p>
    Given these weights, we now want to calculate a probability distribution for
    each example in our training set. We need a way to pluck out the weights row
    for each input (x) in the dataset. For example <code>x[0]</code> is
    <code>0</code> and needs to pluck out the 0th row of our weights, which can
    then give us the probability distribution for the next character. Our neural
    net will later on change the weights so the probability for
    <code>y[0]</code> (E) to be a starting character gets higher.
</p>
<p>
    An easy way to do this is to convert <code>xs</code> to a one-hot matrix
    (<code>xs.length</code> x <code>totalChars</code>) and then matrix multiply
    that with the weights (<code>totalChars</code> x <code>totalChars</code>)
    resulting in the desired <code>xs.length</code> x <code>totalChars</code>
    matrix where each row is the weights for the character in <code>xs</code>.
    Since the one-hot matrix is all zeros except for one column, it effectively
    plucks out the row with that column index from the weights matrix.
</p>
<p>
    First, let’s one-hot encode <code>xs</code>.
</p>
<script src>
    export function oneHot( a, length ) {
        const B = new FloatMatrix( null, [ a.length, length ] );
        for ( let i = a.length; i--; ) B[ i * length + a[ i ] ] = 1;
        return B;
    }

    export const XOneHot = oneHot( xs, indexToCharMap.length );
</script>
<p>
    Now let’s multiply it by the weights. For that we need to implement matrix
    multiplication.
</p>
<script src>
    export function matMul(A, B) {
        const [ m, n ] = A.shape;
        const [ p, q ] = B.shape;
        const C = new FloatMatrix( null, [ m, q ] );

        if ( n !== p ) {
            throw new Error( 'Matrix dimensions do not match.' );
        }

        for ( let m_ = m; m_--; ) {
            for ( let q_ = q; q_--; ) {
                let sum = 0;
                for ( let n_ = n; n_--; ) {
                    sum += A[m_ * n + n_] * B[n_ * q + q_];
                }
                C[m_ * q + q_] = sum;
            }
        }

        return C;
    }

    export const Wx = matMul( XOneHot, W );
</script>
<p>
    This is the only layer we will have in our neural network for now. Just a
    linear layer without a bias, a really simple and dumb neural network.
</p>
<p>
    Now we have the weights for each input, but we somehow want these numbers to
    represent the probabilities for the next character. For this we can use the
    softmax, which scales numbers into probabilities. So for each row of
    <code>Wx</code> we want to call the softmax function so that each row is a
    probability distribution summing to 1. You can see <code>Wx</code> a bit as
    “log-counts” (also called “logits”) that can be exponentiated to get fake
    “counts” (the occurrences of a character for a previous character). To get
    probabilities, they can be normalised by dividing over the sum. This is
    basically what softmax does.
</p>
<p>
    First we need to create a softmax function. We want to calculate the softmax
    for every row. We’re subtracting the maximum before exponentiating for
    numerical stability.
</p>
<script src>
    export function softmaxByRow( A ) {
        const [m, n] = A.shape;
        const B = new FloatMatrix( null, A.shape );
        for ( let m_ = m; m_--; ) {
            let max = -Infinity;
            for ( let n_ = n; n_--; ) {
                const value = A[m_ * n + n_];
                if (value > max) max = value;
            }
            let sum = 0;
            for ( let n_ = n; n_--; ) {
                const i = m_ * n + n_;
                // Subtract the max to avoid overflow
                sum += B[i] = Math.exp(A[i] - max);
            }
            for ( let n_ = n; n_--; ) {
                B[m_ * n + n_] /= sum;
            }
        }
        return B;
    }

    export const probs = softmaxByRow( Wx );
</script>
<p>
    Now that we have probabilities for each row (summing to 1), we can use this
    later to sample from the model.
</p>
<h2>
    Evaluating the model
</h2>
<p>
    We now need a way evaluate the model mathematically. We can do this by
    creating a function to calculate a loss. As long as it’s all differentiable,
    we can then calculate the gradients with respect to the weights matrix and
    tune it, giving us a better loss and thus better probabilities. The idea is
    that we can keep doing this and iteratively tune the weights matrix to give
    one with as low a loss as possible.
</p>
<p>
    The loss function we will use here is called the negative log likelihood,
    which measures how well the data fits our model.
</p>
<p>
    First, we need to pluck out the probabilities given the characters in
    <code>y</code>, also called the labels or targets. If you do this with
    untrained weights, you’ll notice that these probabilities are not very good.
    Then we calculate the log probabilities or likelihoods by taking the log of
    each probability. To get the log likelihood of everything together, we take
    the mean.
</p>
<p>
    Here’s how to do it in JS in a single loop.
</p>
<script src>
    export function negativeLogLikelihood( probs, ys ) {
        const [m, n] = probs.shape;
        let sum = 0;
        for ( let m_ = m; m_--; ) {
            // Sum the logProbs (log likelihoods) of the correct label.
            sum += Math.log( probs[ m_ * n + ys[ m_ ] ] );
        }
        const mean = sum / m;
        // Mean negative log likelihood.
        return - mean;
    }

    // Let's keep track of the losses.
    export const loss = negativeLogLikelihood( probs, ys );
    export const losses = [ loss ];
</script>
<h2>
    Backward pass to calculate gradients
</h2>
<p>
    Now we want to calculate the gradients for the weights matrix. We’ll split
    this into two: first calculate the gradients for <code>Wx</code>, and then
    from those the gradients for <code>W</code>.
</p>
<p>
    Now this is not explained in Karpathy’s <a
    href="https://href.li/?https://www.youtube.com/watch?v=PaCmpygFfXo">second
    video</a>, he’s just using PyTorch’s backward. We can’t do that cause this
    is hardcore from scratch! Fortunately, when using the softmax activation
    combined with cross-entropy loss, the gradient simplifies to the difference
    between our predicted probabilities and the actual labels, which he explains
    in <a
    href="https://href.li/?https://youtu.be/q8SA3rM6ckI?si=vXBKdMh7sSO44VJT&amp;t=5187"
    >part 4</a>. This is only for a single example, so when spreading the
    gradient we also need to divide by the number of rows.
</p>
<script src>
    export function softmaxCrossEntropyGradient( probs, ys ) {
        const [m, n] = probs.shape;
        const gradient = new FloatMatrix( probs );
        for ( let m_ = m; m_--; ) {
            // Subtract 1 for the gradient of the correct label.
            gradient[ m_ * n + ys[ m_ ] ] -= 1;
            for ( let n_ = n; n_--; ) {
                // Divide by the number of rows.
                gradient[ m_ * n + n_ ] /= m;
            }
        }
        return gradient;
    }

    export const WxGradient = softmaxCrossEntropyGradient( probs, ys );
</script>
<p>
    Now the easier one: the derivative of a matrix multiplication <code>A *
    B</code> with respect to the second parameter is <code>dB = A^T *
    dAB</code>.
</p>
<script src>
    export function transpose( A ) {
        const [ m, n ] = A.shape;
        const B = new FloatMatrix( null, [ n, m ] );

        for ( let m_ = m; m_--; ) {
            for ( let n_ = n; n_--; ) {
                B[n_ * m + m_] = A[m_ * n + n_];
            }
        }

        return B;
    }

    export const WGradient = matMul( transpose( XOneHot ), WxGradient );
</script>
<p>
    And finally, we can update the weights using these gradients! Let’s first
    set a learning rate of 10.
</p>
<script src>
    export const learningRate = 10;
</script>
<script src>
    for ( let i = W.length; i--; ) W[ i ] -= learningRate * WGradient[ i ];
    print( W );
</script>
<p>
    If we now calculate the loss again, it should be lower!
</p>
<script src>
    const newProbs = softmaxByRow( matMul( XOneHot, W ) );
    export const newLoss = negativeLogLikelihood( newProbs, ys );
    print( loss, 'oldLoss' );
</script>
<h2>
    Iterate
</h2>
<p>
    Now we need to iterate over this many times.
</p>
<script src>
    export async function iteration() {
        const Wx = await matMul( XOneHot, W );
        const probs = softmaxByRow( Wx );

        losses.push( negativeLogLikelihood( probs, ys ) );

        // Backpropagation.
        const WxGradient = softmaxCrossEntropyGradient( probs, ys );
        const WGradient = await matMul( transpose( XOneHot ), WxGradient );

        for ( let i = W.length; i--; ) W[ i ] -= learningRate * WGradient[ i ];
    }
</script>
<p>
    Just so we can visualise this better, let’s create two graphs using <a
    href="https://plotly.com/javascript/">Plotly</a>:
</p>
<ol>
    <li>
        The losses for each iteration.
    </li>
    <li>
        A table with all bigram combinations, darker means a higher occurrence.
    </li>
</ol>
<script src>
    export { default as Plotly } from 'https://cdn.jsdelivr.net/npm/plotly.js-dist@2.26.2/+esm';
    export const graphs = document.createElement('div');
    graphs.append( document.createElement('div') );
    graphs.append( document.createElement('div') );
    graphs.style.display = 'flex';
</script>
<p>
    For each iteration, let’s update the graphs.
</p>
<script src data-iterations="10" id="iteration">
    await iteration();
    await Plotly.react(
        graphs.firstChild,
        [ { x: losses.map( ( _, i ) => i ), y: losses } ],
        {
            width: 500, height: 500,
            yaxis: { title: 'Loss', type: 'log' },
            xaxis: { title: 'Iterations' }
        },
        { displayModeBar: false }
    );
    const r = indexToCharMap.map( ( _, i ) => i )
    await Plotly.react(
        graphs.lastChild,
        [ {
            x: indexToCharMap, y: indexToCharMap,
            z: r.map((_, m_) => r.map((_, n_) => Math.exp(W[m_ * r.length + n_]))),
            type: 'heatmap', showscale: false,
            colorscale: [ [ 0, 'white' ], [ 1, 'black' ] ],
        } ],
        {
            width: 500, height: 500,
            yaxis: { tickvals: [], autorange: 'reversed' },
            xaxis: { tickvals: [], },
            margin: { t: 10, b: 10, l: 10, r: 10 },
            annotations: r.map((_, m_) => r.map((_, n_) => ({
                x: indexToCharMap[n_],
                y: indexToCharMap[m_],
                text: `${indexToCharMap[m_]}${indexToCharMap[n_]}`,
                showarrow: false,
                font: { color: 'white' }
            }))).flat(),
        },
        { displayModeBar: false }
    );
    export default graphs;
</script>
<p>This works, but it’s very slow!</p>
<h2>
    Bonus! GPU!
</h2>
<p>
    Running matrix multiplication on the CPU is slow. Even running it
    unoptimised on the GPU will be much faster. It took me a while to recreate
    matrix multiplication with WebGPU. After crashing my computer a few times (I
    swear it did!), I ended up with some code that works. Here’s the <a
    href="https://href.li/?https://github.com/ellatrix/micrograd/blob/main/matmul-gpu.js"
    >code</a>, and the <a
    href="https://href.li/?https://github.com/ellatrix/micrograd/blob/main/matmul.wgsl"
    >shader</a>. No external libraries, so we’re not cheating. This only
    works in Chrome and Edge though.</p>
<script src>
    const { GPU } = await import( new URL( './matmul-gpu.js', location ) );
    export const matMul = ( await GPU() )?.matMul || matMul;
</script>
<p>
    Let's now go <a href="#iteration">back to iterating</a>, set it to
    <mark>200</mark>, and run it again until the loss is below <mark>2.5</mark>.
</p>
<h2>
    Sampling
</h2>
<p>
    To sample from the model, we need to input a single character that is
    one-hot encoded. Matrix multiply that with the weights, then calculate the
    probabilities as before. Then we can take a random number between 0 and 1
    and pick from the probabilities. Higher probabilities have a larger “surface
    area” in the sample function.
</p>
<script src data-iterations="10">
    function sample(probs) {
        const sample = Math.random();
        let total = 0;
        for ( let i = probs.length; i--; ) {
            total += probs[ i ];
            if ( sample < total ) return i;
        }
    }

    const indices = [ 0 ];

    do {
        const context = indices.slice( -1 );
        const Wc = await matMul( oneHot( context, indexToCharMap.length ), W );
        const probs = softmaxByRow( Wc );
        indices.push( sample( probs ) );
    } while ( indices[ indices.length - 1 ] );

    export const name = indices.slice( 1, -1 ).map( ( i ) => indexToCharMap[ i ] ).join( '' );
</script>
<p>
    The output is not great, but it's also not completely random. If you're
    curious, run this notebook again without training and you'll see that
    sampling with untrained weights is completely random.
</p>
<p>
    In the next notebook we'll see how to enlarge the network, get a better
    loss, and better samples.
</p>
<script>
    const scripts = [ ...document.querySelectorAll('script[src=""]') ];

    scripts.forEach( ( script ) => {
        const outputwrapper = document.createElement('div');
        const div = document.createElement('details');
        div.open = true;
        const button = document.createElement('button');
        button.innerText = 'Run';
        const pre = document.createElement('textarea');
        const iInput = document.createElement('input');
        const float = document.createElement('summary');
        float.tabIndex = -1;
        iInput.type = 'number';
        iInput.value = script.dataset.iterations;

        div.onkeydown = ( event ) => {
            if ( event.key === 'Enter' && event.shiftKey ) {
                event.preventDefault();
                button.click();
            }
        };

        function stringify( data ) {
            if ( window.FloatMatrix && data instanceof FloatMatrix ) {
                if ( data.shape.length === 1 ) return stringify( Array.from( data ) );

                // If larger than 6 rows, get the first 3 and last 3.
                if ( data.shape[ 0 ] > 6 ) {
                    const rows = [];
                    for ( let m = 0; m < 3; m++ ) {
                        const row = [];
                        for ( let n = 0; n < data.shape[ 1 ]; n++ ) {
                            row.push( data[ m * data.shape[ 1 ] + n ] );
                        }
                        rows.push( `[ ${ row.join(', ') } ]` );
                    }
                    rows.push( '...' );
                    for ( let m = data.shape[ 0 ] - 3; m < data.shape[ 0 ]; m++ ) {
                        const row = [];
                        for ( let n = 0; n < data.shape[ 1 ]; n++ ) {
                            row.push( data[ m * data.shape[ 1 ] + n ] );
                        }
                        rows.push( `[ ${ row.join(', ') } ]` );
                    }
                    return `${data.shape.join('×')} [
 ${ rows.join(',\n ') }
]`;
                }

                const rows = [];
                for ( let m = 0; m < data.shape[ 0 ]; m++ ) {
                    const row = [];
                    for ( let n = 0; n < data.shape[ 1 ]; n++ ) {
                        row.push( data[ m * data.shape[ 1 ] + n ] );
                    }
                    rows.push( `[ ${ row.join(', ') } ]` );
                }
                return `${data.shape.join('×')} [
 ${ rows.join(',\n ') }
]`;
            }

            function hellip( string, condition ) {
                return condition ? `${string.slice(0,-1)}…` : string;
            }

            if ( typeof data === 'string' ) return hellip( JSON.stringify( data.slice( 0, 100 ) ), data.length > 100 );
            if ( typeof data === 'number' ) return data.toString();
            if ( typeof data === 'boolean' ) return data.toString();
            if ( typeof data === 'undefined' ) return 'undefined';
            if ( data === null ) return 'null';
            if ( data instanceof Error ) return data.toString();
            if ( data instanceof Array || data instanceof Float32Array ) {
                const string = `${data.length} ${ JSON.stringify( Array.from( data ).slice(0,100), null, 1 ).replace( /\n\s*/g, ' ' ) }`
                return hellip( string, data.length > 100 );
            }
            if ( typeof data === 'object' ) return JSON.stringify( data, ( key, value ) => {
                if ( ! key ) return value;
                if ( typeof value === 'function' ) return '[Function]';
                if ( typeof value === 'object' ) return '[Object]';
                return value;
            }, 1 ).replace( /\n\s*/g, ' ' );
            if ( typeof data === 'function' ) return `Function`;
        }

        let output;

        button.tabIndex = -1;
        button.onclick = async () => {
            div.open = false;
            button.disabled = true;
            output?.remove();
            output = document.createElement('pre');
            outputwrapper.append( output );
            outputwrapper.focus();
            pre?.editor.save();
            const blob = new Blob( [ pre.value ], { type: 'text/javascript' } );
            print = function ( data, key = '' ) {
                const callback = () => {
                    const line = document.createElement('div');
                    if ( data instanceof Element ) {
                        line.appendChild( data );
                    } else {
                        if ( key ) {
                            const b = document.createElement('b');
                            b.textContent = key;
                            line.appendChild( b );
                        }
                        line.appendChild(
                            document.createTextNode( ( key ? ': ' : '' ) + stringify( data ) )
                        );
                    }
                    output.appendChild( line );
                }

                if ( key ) {
                    callback();
                } else {
                    requestAnimationFrame( callback );
                }
            }

            let i = parseInt( iInput.value, 10 ) || 1;

            const promiseExecutor = (resolve, reject) => {
                const callback = async () => {
                    if (i--) {
                        const url = URL.createObjectURL(blob);
                        try {
                            const imports = await import(url);
                            Object.keys(imports).forEach((key) => {
                                window[key] = imports[key];
                                print(imports[key], key);
                            });
                            requestAnimationFrame(callback);
                        } catch (error) {
                            output.dataset.error = true;
                            print(error);
                            resolve();
                        }
                    } else {
                        resolve();
                    }
                }

                callback();
            };

            await new Promise( promiseExecutor );
            button.disabled = false;
        };

        div.onfocus = () => {
            div.open = true;
        };

        pre.button = button;
        const indentation = script.innerText.match( /^\s*/ )[ 0 ];
        pre.style.width = '100%';
        pre.value = script.innerText.replace( new RegExp( indentation, 'g' ), '\n' ).trim();
        pre.rows = pre.value.split( '\n' ).length;
        iInput.style.width = '4em';
        float.appendChild( button );
        if ( script.dataset.iterations !== undefined ) {
            float.appendChild( document.createTextNode( ' × ' ) );
            float.appendChild( iInput );
        }
        
        div.appendChild( float );
        div.appendChild( pre );
        div.id = script.id;
        script.replaceWith( div );
        div.after( outputwrapper );
    } );

    [...document.body.children].forEach( ( block ) => {
        block.tabIndex = 0;
        block.setAttribute( 'aria-label', 'Shift+Enter to continue' );
    } );

    document.body.addEventListener('keydown', ( event ) => {
        if ( event.key === 'Enter' && event.shiftKey && ! event.defaultPrevented ) {
            document.activeElement.closest('[aria-label]').nextElementSibling?.focus();
        }
    })

    document.body.firstElementChild.focus();
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/codemirror.min.css" integrity="sha512-uf06llspW44/LZpHzHT6qBOIVODjWtv4MxCricRxkzvopAlSWnTf6hpZTFxuuZcuNE9CBQhqE0Seu1CoRk84nQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/codemirror.min.js" integrity="sha512-8RnEqURPUc5aqFEN04aQEiPlSAdE0jlFS/9iGgUyNtwFnSKCXhmB6ZTNl7LnDtDWKabJIASzXrzD0K+LYexU9g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/mode/javascript/javascript.min.js" integrity="sha512-I6CdJdruzGtvDyvdO4YsiAq+pkWf2efgd1ZUSK2FnM/u2VuRASPC7GowWQrWyjxCZn6CT89s3ddGI+be0Ak9Fg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<style>
    .CodeMirror, .CodeMirror-scroll {
        height: auto;
        background: none;
    }
</style>
<script>
    document.querySelectorAll('textarea').forEach((textarea) => {
        textarea.editor = CodeMirror.fromTextArea(textarea, {
            mode: 'javascript',
            viewportMargin: Infinity,
            // theme: 'material',
            extraKeys: {
                'Shift-Enter': (cm) => {
                    textarea.button.focus();
                },
            },
        });
    })
</script>
